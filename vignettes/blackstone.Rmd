---
title: "Introduction to blackstone"
output: rmarkdown::html_vignette
description: >
  Start here if this is your first time using blackstone.
vignette: >
  %\VignetteIndexEntry{Introduction to blackstone}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup,  echo = FALSE, message = FALSE, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE, 
  message = FALSE, 
  fig.width = 9,
  fig.height = 8,
  out.width = "100%"
)
options(scipen = 999, digits = 4, # Prevents sci notation and sets the output of decimals to 4 (0.0000)
        tibble.print_min = 5L, tibble.print_max = 5L) # tibble only print 5 rows 
library(blackstone) # load Blackstone R package
set.seed(424629) # set reproducible seed
```


Blackstone Research and Evaluation uses the `R` programming language as its main tool for data analysis and visualization.
The main goal of `blackstone` is to make the process of coding in `R` as efficient as possible for everyone at Blackstone Research and Evaluation. 

In order to reach this goal, `blackstone` leverages established `R` packages and the [tidyverse](https://www.tidyverse.org/) family of `R` packages in particular. 
These packages use the system of "tidy data" to make data manipulation and analysis in `R` consistent and well organized, `blackstone` also strives to follow this philosophy. 

The vignettes provided with `blackstone` will go into greater detail as to how to use `blackstone` alongside these other packages.
This document introduces you to `blackstone` and how it fits into the basic workflow of data analysis and visualization at Blackstone Research and Evaluation.

## Overview of Data Workflow

* Import and clean data from survey provider.

* Simple data analysis and statistical inference.

* Data visualization and output for report writing.

## Importing data from SurveyMonkey

Our current survey provider is [SurveyMonkey](https://www.surveymonkey.com/?ut_source=homepage&ut_source3=header), `blackstone` contains several functions that makes the process of 
reading SurveyMonkey data into `R` a more manageable process and creates a codebook for the data along the way.

SurveyMonkey exports data with two header rows, which does not work with `R`, where tibbles and dataframes can only have one row of names.

Here is how to import data from SurveyMonkey using example data provided with `blackstone`, this is a fake dataset of a pre (baseline) survey.

There are three steps to this process:

1. Create a codebook.

```{r create_codebook}
# File path for pre example data:
pre_data_fp <- blackstone::blackstoneExample("sm_data_pre.csv")
# 1. Create the codebook:
codebook_pre <- blackstone::createCodebook(pre_data_fp)
codebook_pre
```
For this codebook, the first column `header_1` is the first header from the SurveyMonkey data, the second column `header_2` is the second header,
the third column `combined_header` is the combination of the two headers,
`position` is the column number position for each `combined_header`, and `variable_name` is a cleaned up version for `combined_header` and
will be the column to edit to change the column names later on to shorter and more meaningful names.

`variable_name` will be the column that renames all the variables in the SurveyMonkey data.

2. Edit the codebook to create meaningful variable names.

```{r edit_codebook}
# Step 2. Edit the codebook: 
# Set up sequential naming convections for matrix-style questions with shared likert scale response options:
# 8 items that are matrix-style likert scales- turned into a scale called `research`- here is how to easily name them all at once:
# Rows 11 to 18 belong to the "research" matrix question (you will have to look at the codebook and match the header_1 and header_2 to variable_name to change)
research_items <- codebook_pre[["variable_name"]][11:18]
research_names <- paste0("research_", seq_along(research_items)) %>% purrr::set_names(., research_items) # Create a new named vector of names for these columns
# 6 items that are matrix-style likert scales- turned into a scale called `ability`- Rows 19 to 24 named `variable_name`:
ability_items <- codebook_pre[["variable_name"]][19:24]
ability_names <- paste0("ability_", seq_along(ability_items)) %>% purrr::set_names(., ability_items) # Create a new named vector of names for these columns
# 6 items that are matrix-style likert scales- turned into a scale called `ethics`- Rows 19 to 24 named `variable_name`:
ethics_items <- codebook_pre[["variable_name"]][25:29]
ethics_names <- paste0("ethics_", seq_along(ethics_items)) %>% purrr::set_names(., ethics_items) # Create a new named vector of names for these columns
# Edit the `variable_names` column: Use dplyr::mutate() and dplyr::case_match() to change the column `variable_name`:
codebook_pre <- codebook_pre %>% dplyr::mutate(
    variable_name = dplyr::case_match(
        variable_name, # column to match
        'custom_data_1' ~ "unique_id", # changes 'custom_data_1' to "unique_id"
        'to_what_extent_are_you_knowledgeable_in_conducting_research_in_your_field_of_study' ~ "knowledge",
        'with_which_gender_do_you_most_closely_identify' ~ "gender",
        'which_race_ethnicity_best_describes_you_please_choose_only_one' ~ "ethnicity",
        'are_you_a_first_generation_college_student' ~ "first_gen",
        names(research_names) ~ research_names[variable_name], # takes the above named vector and when the name matches, applies new value in that position as replacement.
        names(ability_names) ~ ability_names[variable_name],   # Same for `ability_names`
        names(ethics_names) ~ ethics_names[variable_name],   # Same for `ability_names`
        .default = variable_name # returns default value from original `variable_name` if not changed.
        )
    )
codebook_pre
# Write out the edited codebook to save for future use-
# Be sure to double check questions match new names before writing out:
# readr::write_csv(codebook_pre, file = "{filepath-to-codebok}")
```

3. Read in the data and rename the variables with the codebook.

```{r import}
# 3. Read in the data and rename the vars using readRenameData(), passing the file path and the edited codebook:
pre_data <- blackstone::readRenameData(pre_data_fp, codebook = codebook_pre)
pre_data
```

The SurveyMonkey example data is now imported with names taken from the codebook column `variable_name`:

```{r data_names}
names(pre_data)
```

```{r, include=FALSE}
# TODO add link to vignette below to 'vignette/import_clean.Rmd'
```

The vignette `Importing and Cleaning Data` goes into deeper detail with example data on how to use `blackstone` to import and clean data.


## Data Analysis and Statistical Inference

First, read in the data that is merged and cleaned in the vignette `Importing and Cleaning Data`:

```{r}
# Read in clean SM data:
sm_data <- readr::read_csv(blackstone::blackstoneExample("sm_data_clean.csv"), show_col_types = FALSE)

## Set up character vectors of likert scale levels:
## Knowledge scale
levels_knowledge <- c("Not knowledgeable at all", "A little knowledgeable", "Somewhat knowledgeable", "Very knowledgeable", "Extremely knowledgeable")
## Research Items scale:
levels_confidence <- c("Not at all confident", "Slightly confident", "Somewhat confident", "Very confident", "Extremely confident")
## Ability Items scale: 
levels_min_ext <- c("Minimal", "Slight", "Moderate", "Good", "Extensive")
## Ethics Items scale:
levels_agree5 <- c("Strongly disagree", "Disagree", "Neither agree nor disagree", "Agree", "Strongly agree")

# Demographic levels:
gender_levels <- c("Female","Male","Non-binary", "Do not wish to specify")
ethnicity_levels <- c("White (Non-Hispanic/Latino)", "Asian", "Black",  "Hispanic or Latino", "American Indian or Alaskan Native",
                      "Native Hawaiian or other Pacific Islander", "Do not wish to specify")
first_gen_levels <- c("Yes", "No", "I'm not sure")

# Use mutate() for convert each item in each scale to a factor with vectors above, across() will perform a function for items selected using contains() or can be selected 
# by variables names individually using a character vector: _knowledge or use c("pre_knowledg","post_knowledge")
# Also create new numeric variables for all the likert scale items and use the suffix '_num' to denote numeric:
sm_data <- sm_data %>% dplyr::mutate(dplyr::across(tidyselect::contains("_knowledge"), ~ factor(., levels = levels_knowledge)), # match each name pattern to select to each factor level
                                     dplyr::across(tidyselect::contains("_knowledge"), as.numeric, .names = "{.col}_num"), # create new numeric items for all knowledge items
                                     dplyr::across(tidyselect::contains("research_"), ~ factor(., levels = levels_confidence)), 
                                     dplyr::across(tidyselect::contains("research_"), as.numeric, .names = "{.col}_num"), # create new numeric items for all research items
                                     dplyr::across(tidyselect::contains("ability_"), ~ factor(., levels = levels_min_ext)),
                                     dplyr::across(tidyselect::contains("ability_"), as.numeric, .names = "{.col}_num"), # create new numeric items for all ability items
                                     # select ethics items but not the open_ended responses:
                                     dplyr::across(tidyselect::contains("ethics_") & !tidyselect::contains("_oe"), ~ factor(., levels = levels_agree5)),
                                     dplyr::across(tidyselect::contains("ethics_") & !tidyselect::contains("_oe"), as.numeric, .names = "{.col}_num"), # new numeric items for all ethics items
                                     # individually convert all demographics to factor variables:
                                     gender = factor(gender, levels = gender_levels),
                                     ethnicity = factor(ethnicity, levels = ethnicity_levels),
                                     first_gen = factor(first_gen, levels = first_gen_levels),
                                     )
sm_data
```


### Likert Scale Table:

The most common task is creating frequency tables of counts and percentages for likert scale items, `blackstone` has the `likertTable()` for that:

```{r}
# Research items pre and post frequency table, with counts and percentages: use levels_confidence character vector
# use likertTable to return frequency table, passing the scale_labels: (can also label the individual questions using the arg question_label)
sm_data %>% dplyr::select(tidyselect::contains("research_") & !tidyselect::contains("_num") & where(is.factor)) %>% 
                blackstone::likertTable(., scale_labels = levels_confidence)
```

### Grouped Demograpic table:

`blackstone` contains a function to create frequency tables for demographics that can be grouped by a variable like role or cohort as well: [`groupedTable()`].

```{r demos table}
# Set up labels for variables
# Labels for questions column of table, pass to question_labels argument:
demos_labels <- c('Gender' = "gender",
                  'Race/Ethnicity' = "ethnicity",
                  'First-Generation College Student' = "first_gen")

sm_data %>% dplyr::select(gender, ethnicity, first_gen) %>% # select the demographic vars
                 blackstone::groupedTable(question_labels = demos_labels) # pass the new labels for the 'Question' column.
```


### Statistical Inference- T-test or Wilcoxon test

#### Running normality test on Single pre-post items:

```{r}
# Use a pipe-friendly version of `shapiro_test()` from `rstatix`, need to covert create a differnce score of post_knowledge_num - pre_knowledge_num named `knowledge_diff`:
sm_data %>% dplyr::select(tidyselect::contains("_knowledge") & tidyselect::contains("_num")) %>%  # select knowledge pre and post numeric items
    dplyr::mutate(knowledge_diff = post_knowledge_num - pre_knowledge_num) %>% # get difference of pre and post scores
    rstatix::shapiro_test(knowledge_diff)
```

Data is *not* normally distributed for the knowledge items (since the p-value is < 0.05)- use a Wilcoxon test.

```{r}
# Use a pipe-friendly version of `wilcox_test()` from `rstatix`, need to covert to long form and have `timing` as a variable,
# the column named `p` is the p-value:
sm_data %>% dplyr::select(tidyselect::contains("_knowledge") & tidyselect::contains("_num")) %>% 
            tidyr::pivot_longer(tidyselect::contains(c("pre_", "post_")), names_to = "question", values_to = "response") %>%
            tidyr::separate(.data$question, into = c("timing", "question"), sep = "_", extra = "merge") %>% 
            rstatix::wilcox_test(response ~ timing, paired = TRUE, detailed = TRUE)
```

Wilcoxon test is significant, there is a significant difference in pre and post scores of knowledge scores.

```{r, include=FALSE}
# TODO add link to vignette below to 'vignette/analysis.Rmd'
```

The vignette `Data analysis and Statistical Inference` goes into deeper detail on how to use `blackstone` to perform more analysis and statistical tests on example data.

## Data Visualization

`blackstone` has functions that create 3 types of charts for data visualization: stacked bar charts, diverging stacked bar charts, and arrow charts.

The functions for stacked bar charts and diverging stacked bar charts can use two different color palettes: a blue sequential palette or a blue-red diverging color palette. 

The blue sequential palette should be used for all likert scales that have one clear direction like: `r levels_confidence`

The blue-red diverging color palette should be used if the items have a likert scale that is folded or runs from a negative to positive valence like this: `r levels_agree5` 

```{r, include=FALSE}
# TODO add link to vignette below to 'vignette/data_visualization.Rmd'
```

This introduction will only show how to create a stacked bar chart, see the vignette `Data Visualization` for a full explanation of all the data visualization functions in `blackstone`.

### Stacked Bar Charts

The most common visual that is used with reporting at Blackstone Research and Evaluation is a stacked bar chart, `blackstone` has a function to that makes creating these charts
fast and easy: [`stackedBarChart()`].

`stackedBarChart()` takes in a [tibble](https://tibble.tidyverse.org/) of factor/character variables to turn into a stacked bar chart. The other requirement is a character vector of 
scale labels for the likert scale that makes up the items in the tibble (same as the one use to set them up as factors in the data cleaning section). 

#### Pre-post Stacked Bar Chart with Overall *n* and Percentages:

- By default, `stackedBarChart()` uses the blue sequential palette to color the bars and sorts the items by the ones with the highest post items with the highest counts/percentages.

```{r}
# Research Items scale:
levels_confidence <- c("Not at all confident", "Slightly confident", "Somewhat confident", "Very confident", "Extremely confident")

# select variables and pass them to `stackedBarChart()` along with scale_labels.
sm_data %>% dplyr::select(tidyselect::contains("research_") & !tidyselect::contains("_num") & where(is.factor)) %>% # select the factor variables for the research items
    blackstone::stackedBarChart(., scale_labels = levels_confidence, pre_post = TRUE)

```

```{r, include=FALSE}
# TODO add link to vignette below to 'vignette/data_visualization.Rmd'
```

The vignette `Data Visualization` goes into deeper detail on how to use `blackstone` to create data visualizations with example data .

